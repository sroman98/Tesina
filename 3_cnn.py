# -*- coding: utf-8 -*-
"""3 CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1b9Peuu0BEOjZWuYtCq2uOF7yJsph5eKy

# Setup
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.datasets as datasets
import torchvision.transforms as T
import matplotlib.pyplot as plt

from google.colab import drive
from os import listdir
from os.path import isfile, join
from scipy.io import loadmat
from statistics import mean
from torch.nn import functional as F
from torch.utils.data import DataLoader, Dataset, random_split
from torch.utils.data import sampler

drive.mount('/gdrive')
# %cd /gdrive/MyDrive/Sandra - DL y EEGs para TDAH en niÃ±os/dataset
adhd_paths = ['ADHD_1', 'ADHD_2']
control_paths = ['Control_1', 'Control_2']

def getEEGsFromPaths(paths):
  eegs = []
  
  for path in paths:
    files = [f for f in listdir(path) if isfile(join(path, f))]

    for file in files:
      eeg_mat = loadmat(path + '/' + file)
      size = len(file)
      file_no_ext = file[:size - 4]
      eeg = eeg_mat[file_no_ext]
      eegs.append(eeg)
  
  return eegs

def accuracy(model, loader):
    num_correct = 0
    num_total = 0
    num_cost = 0
    model.eval()
    model = model.to(device=device)
    with torch.no_grad():
        for xi, yi in loader:
            xi = xi.to(device=device, dtype = torch.float)
            yi = yi.to(device=device, dtype = torch.long)
            scores = model(xi) 
            num_cost += (F.cross_entropy(scores, yi)).item()
            _, pred = scores.max(dim=1) 
            num_correct += (pred == yi).sum() 
            num_total += pred.size(0)
        return num_cost/len(loader), float(num_correct)/num_total   

def find_lr(model, optimiser, loader, start_val = 1e-6, end_val = 1, beta = 0.99):
    n = len(loader) - 1
    factor = (end_val / start_val)**(1/n)
    lr = start_val
    optimiser.param_groups[0]['lr'] = lr #this allows you to update the learning rate
    avg_loss, loss, acc = 0., 0., 0.
    lowest_loss = 0.
    batch_num = 0
    losses = []
    log_lrs = []
    accuracies = []
    model = model.to(device=device)
    for i, (x, y) in enumerate(loader, start=1):
        x = x.to(device = device, dtype = torch.float)
        y = y.to(device = device, dtype = torch.long)
        optimiser.zero_grad()
        scores = model(x)
        cost = F.cross_entropy(input=scores, target=y)
        loss = beta*loss + (1-beta)*cost.item()
        avg_loss = loss/(1 - beta**i)
        
        acc_ = ((torch.argmax(scores, dim=1) == y).sum()/scores.size(0)) 
        if i > 1 and avg_loss > 4 * lowest_loss:
            print(f'from here {i, cost.item()}')
            return log_lrs, losses, accuracies
        if avg_loss < lowest_loss or i == 1:
            lowest_loss = avg_loss
        
        accuracies.append(acc_.item())
        losses.append(avg_loss)
        log_lrs.append(lr)
        cost.backward()
        optimiser.step()
        print(f'cost:{cost.item():.4f}, lr: {lr:.4f}, acc: {acc_.item():.4f}')
        lr *= factor
        optimiser.param_groups[0]['lr'] = lr
        
    return log_lrs, losses, accuracies 

def train(model, optimiser, scheduler = None, epochs = 100):
    model = model.to(device = device)
    val_loss_history = []
    train_loss_history = []
    val_acc_history = []
    train_acc_history = []
    lrs = []
    train_cost = 0.
    val_cost = 0.
    train_cost_acum = 0
    for epoch in range(epochs):
        train_correct_num  = 0
        train_total = 0
        train_cost_acum = 0
        for mb, (x, y) in enumerate(train_loader, start=1):
            model.train()
            x = x.to(device=device, dtype=torch.float)
            y = y.to(device=device, dtype=torch.long)
            scores = model(x)
            cost = F.cross_entropy(input=scores, target=y)
            optimiser.zero_grad()
            cost.backward()
            optimiser.step()
            if scheduler: scheduler.step()

            train_correct_num += (torch.argmax(scores, dim=1) == y).sum()    
            train_total += scores.size(0)        
            train_cost_acum += cost.item()
            train_acc = float(train_correct_num)/train_total  
            val_cost, val_acc = accuracy(model, val_loader)

            val_loss_history.append(val_cost)
            train_loss_history.append(cost.item())
            val_acc_history.append(val_acc)
            train_acc_history.append(train_acc)
            lrs.append(optimiser.param_groups[0]["lr"])
        
        train_acc = float(train_correct_num)/train_total
        train_cost = train_cost_acum/len(train_loader)
        print(f'Epoch:{epoch}, train cost: {train_cost:.6f}, val cost: {val_cost:.6f},'
                      f' train acc: {train_acc:.4f}, val acc: {val_acc:4f}, total: {train_total},'
                      f' lr: {optimiser.param_groups[0]["lr"]:.6f}')
        
    return train_loss_history, val_loss_history, train_acc_history, val_acc_history, lrs

adhd_eegs = getEEGsFromPaths(adhd_paths)
control_eegs = getEEGsFromPaths(control_paths)

"""# Format Dataset"""

CHAN = 19
FREQ = 128 # samples / second
seconds_in_split = 1 # can change this to test
SAMPLES = FREQ * seconds_in_split

def split_list(l: list) -> np.array:
  splitedSize = SAMPLES
  wasted = len(l) % splitedSize
  return np.array([l[x:x+splitedSize] for x in range(0, len(l) - wasted, splitedSize)])

adhd_splited_arrays = np.concatenate([split_list(eeg) for eeg in adhd_eegs]) 
control_splited_arrays = np.concatenate([split_list(eeg) for eeg in control_eegs]) 

print(f'ADHD splited: {adhd_splited_arrays.shape}')
print(f'Control splited: {control_splited_arrays.shape}')

class CustomEEGDataset(Dataset):
    def __init__(self, adhd_data, control_data, transform=None, target_transform=None):
      adhd_y = [1] * len(adhd_data)
      control_y = [0] * len(control_data)

      y = np.concatenate((control_y, adhd_y), axis = 0)
      self.eeg_labels = y

      x = np.concatenate((control_splited_arrays, adhd_splited_arrays), axis = 0)
      x = np.transpose(x, (0, 2, 1))
      self.eegs = x

      self.transform = transform
      self.target_transform = target_transform

    def __len__(self):
        return len(self.eeg_labels)

    def __getitem__(self, idx):
        eeg = self.eegs[idx]
        label = self.eeg_labels[idx]
        if self.transform:
            eeg = self.transform(eeg)
        if self.target_transform:
            label = self.target_transform(label)
        return eeg, label

dataset =  CustomEEGDataset(adhd_splited_arrays, control_splited_arrays)

SIZE = len(dataset)
TRAIN_SIZE = int(SIZE * 0.7)
VAL_SIZE = int(SIZE * 0.15)
TEST_SIZE = SIZE - TRAIN_SIZE - VAL_SIZE
MINIBATCH_SIZE = 64

train_dataset, val_dataset, test_dataset = random_split(dataset, [TRAIN_SIZE, VAL_SIZE, TEST_SIZE])

train_loader = DataLoader(train_dataset, batch_size=MINIBATCH_SIZE, shuffle = True)
val_loader = DataLoader(val_dataset, batch_size=MINIBATCH_SIZE, shuffle = True)
test_loader = DataLoader(test_dataset, batch_size=MINIBATCH_SIZE, shuffle = True)

for i, (x, y) in enumerate(test_loader):
    print(i, x.shape, y.shape)

"""# Set Device"""

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)

"""# Plot"""

rnd_sample_idx = np.random.randint(len(test_loader.dataset))
print(f'La imagen muestreada representa un: {test_loader.dataset[rnd_sample_idx][1]}')
image = test_loader.dataset[rnd_sample_idx][0]
image = (image - image.min()) / (image.max() -image.min() )
for i, channel in enumerate(image):
  print(f"Channel {i + 1}")
  plt.figure(figsize=(10,3))
  plt.plot(channel)
  plt.show()

"""#Find LR"""

in_channels = 19
channel1 = 76
channel2 = 152

cnnmodel = nn.Sequential(nn.Conv1d(in_channels=in_channels, out_channels=channel1, 
                                   kernel_size=3, padding=1),
                         nn.ReLU(),
                         nn.Conv1d(in_channels=channel1, out_channels=channel2,
                                    kernel_size= 3, padding=1),
                         nn.ReLU(),
                         nn.MaxPool1d(2, 2),
                         nn.Flatten(), 
                        #  nn.AdaptiveAvgPool1d(channel2), (en vez del flatten para ahorrar espacio)
                        # hacer flatten si es necesario para mandarlo al lineal de solo 152
                         nn.Linear(in_features=64*channel2, out_features=2)                        
                        ) # 128 samples / 2 por el max pool
optimiser = torch.optim.Adam(cnnmodel.parameters(), 0.0001)

lg_lr, losses, accuracies = find_lr(cnnmodel, optimiser, test_loader, start_val=1e-10, end_val=10)

"""#Plot loss"""

f1, ax1 = plt.subplots(figsize=(10,5))
ax1.plot(lg_lr, losses)
ax1.set_xscale('log')
ax1.get_xaxis().get_major_formatter().labelOnlyBase = False
f1.suptitle("Losses")
plt.show()

"""# CNN"""

# cnnmodel = nn.Sequential(nn.Conv1d(in_channels=in_channels, out_channels=channel1, 
#                                    kernel_size=3, padding=1),
#                           nn.ReLU(),
#                           nn.Conv1d(in_channels=channel1, out_channels=channel2,
#                                     kernel_size= 3, padding=1),
#                           nn.ReLU(),
#                           nn.MaxPool1d(2, 2),
#                           nn.Flatten(),
#                           nn.Linear(in_features=8*8*channel2, out_features=2)                        
#                                    ) # not sure of why I did 8x8
# optimiser = torch.optim.Adam(cnnmodel.parameters(), lr)
# train(cnnmodel, optimiser, epochs)
# accuracy(cnnmodel, test_loader)

"""# CNN w/LR"""

epochs = 25
max_lr = 2e-3

cnnmodel = nn.Sequential(nn.Conv1d(in_channels=in_channels, out_channels=channel1, 
                                   kernel_size=3, padding=1),
                          nn.ReLU(),
                          nn.Conv1d(in_channels=channel1, out_channels=channel2,
                                    kernel_size= 3, padding=1),
                          nn.ReLU(),
                          nn.MaxPool1d(2, 2),
                          nn.Flatten(),
                          nn.Linear(in_features=64*channel2, out_features=2)                        
                                   )
optimiser = torch.optim.Adam(cnnmodel.parameters(), 0.0001)

scheduler = torch.optim.lr_scheduler.OneCycleLR(optimiser, 
                                                max_lr=max_lr,
                                                steps_per_epoch=len(train_loader), 
                                                epochs = epochs, pct_start=0.43, 
                                                div_factor=10, 
                                                final_div_factor=1000, 
                                                three_phase=True, verbose=False
                                            )
train_loss_history, val_loss_history, train_acc_history, val_acc_history, lrs = train(
                                cnnmodel, 
                                optimiser=optimiser,
                                scheduler=scheduler,
                                epochs = epochs
                                )

accuracy(cnnmodel, test_loader)

"""# Save model
I saw this [here](https://programmerclick.com/article/18561340729/).
"""

torch.save(cnnmodel.state_dict(), 'checkpoint.pth')

in_channels = 19
channel1 = 76
channel2 = 152
model1 = nn.Sequential(nn.Conv1d(in_channels=in_channels, out_channels=channel1, 
                                   kernel_size=3, padding=1),
                          nn.ReLU(),
                          nn.Conv1d(in_channels=channel1, out_channels=channel2,
                                    kernel_size= 3, padding=1),
                          nn.ReLU(),
                          nn.MaxPool1d(2, 2),
                          nn.Flatten(),
                          nn.Linear(in_features=16*4*channel2, out_features=2)
)

model1.load_state_dict(torch.load('checkpoint.pth'))

accuracy(model1, test_loader)