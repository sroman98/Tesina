# -*- coding: utf-8 -*-
"""1 dataset_playground.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dCFsM9MlaNlz1lCtem_uaDtzFI0olSym

# Mount Google Drive
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/gdrive')
# %cd /gdrive/MyDrive/Sandra - DL y EEGs para TDAH en niños/dataset

"""# Show available files"""

!ls

"""# Load data"""

from scipy.io import loadmat

from os import listdir
from os.path import isfile, join

from statistics import mean

adhd_paths = ['ADHD_1', 'ADHD_2']
control_paths = ['Control_1', 'Control_2']

def getEEGsFromPaths(paths):
  eegs = []
  
  for path in paths:
    files = [f for f in listdir(path) if isfile(join(path, f))]

    for file in files:
      eeg_mat = loadmat(path + '/' + file)
      size = len(file)
      file_no_ext = file[:size - 4]
      eeg = eeg_mat[file_no_ext]
      eegs.append(eeg)
  
  return eegs

def printEEGsShape(eegs):
  for eeg in eegs:
    print(eeg.shape)

adhd_eegs = getEEGsFromPaths(adhd_paths)
print(f'ADHD EEGs: {len(adhd_eegs)}')
printEEGsShape(adhd_eegs)

control_eegs = getEEGsFromPaths(control_paths)
print(f'\nControl EEGs: {len(control_eegs)}')
printEEGsShape(control_eegs)

"""# Analyze data"""

import datetime

CHAN = 19 # 19 canales por sample
FREQ = 128 # Hz (samples / segundo ??)

def max_list(list):
  list_len = [len(i) for i in list]
  return max(list_len)

def min_list(list):
  list_len = [len(i) for i in list]
  return min(list_len)

def avg_list_len(list):
  list_len = [len(i) for i in list]
  return round(mean(list_len))

def samples_to_time(samples):
  secs = samples / FREQ
  return str(datetime.timedelta(seconds=secs))



print('ADHD')
print(f'Shortest: {min_list(adhd_eegs)} samples')
print(f'Longest: {max_list(adhd_eegs)} samples')
adhd_avg = avg_list_len(adhd_eegs)
print(f'Avg: {adhd_avg} samples -> Time: {samples_to_time(adhd_avg)}')

print('\nControl')
print(f'Shortest: {min_list(control_eegs)} samples')
print(f'Longest: {max_list(control_eegs)} samples')
control_avg = avg_list_len(control_eegs)
print(f'Avg: {control_avg} samples -> Time: {samples_to_time(control_avg)}')

"""# Determine length of intervals"""

import numpy as np

wasted_data = []

for i in range(1, 31):
  number_of_samples = list(map(lambda list: len(list) % (FREQ * i), adhd_eegs))
  number_of_samples += list(map(lambda list: len(list) % (FREQ * i), control_eegs))
  wasted_data.append(np.average(number_of_samples) / FREQ)

print("LOST SECONDS / EEG")
wasted_data

"""Selected i = 9 because it's the largest no. of seconds that results in avg loss of less than 5 seconds

### Samples needed per interval = 1,152
9 sec * 128 samples/sec

# Cut EEGs into 9s pieces
"""

SAMPLES = 1152

def split_list(l: list) -> np.array:
  splitedSize = SAMPLES
  wasted = len(l) % splitedSize
  return np.array([l[x:x+splitedSize] for x in range(0, len(l) - wasted, splitedSize)])

adhd_splited_arrays = np.concatenate([split_list(eeg) for eeg in adhd_eegs]) 
control_splited_arrays = np.concatenate([split_list(eeg) for eeg in control_eegs]) 

print(f'ADHD splited: {adhd_splited_arrays.shape}')
print(f'Control splited: {control_splited_arrays.shape}')

"""### Verify shapes make sense"""

def no_pieces(eegs: list) -> int:
  no_pieces = 0
  for eeg in eegs:
    no_pieces += int(len(eeg) / SAMPLES)
  return no_pieces

print(f'Expected shape ADHD: ({no_pieces(adhd_eegs)}, {SAMPLES}, {CHAN})')
print(f'Expected shape control: ({no_pieces(control_eegs)}, {SAMPLES}, {CHAN})')

"""# Label & shuffle ADHD with control data"""

from sklearn import utils

control_y = [0] * len(control_splited_arrays)
adhd_y = [1] * len(adhd_splited_arrays)

y = np.concatenate((control_y, adhd_y), axis = 0)
x = np.concatenate((control_splited_arrays, adhd_splited_arrays), axis = 0)
x = np.transpose(x, (0, 2, 1))

print('SHAPES')
print(f'x {x.shape}')
print(f'y {y.shape}')

"""# Divide data into train, validate & test
total = 1,824
"""

from sklearn.model_selection import train_test_split

x_train_num, x_test_num, y_train_num, y_test_num = train_test_split(x, y, test_size=0.15)

x_train = x_train_num[:1276].reshape(1276, -1)
y_train = y_train_num[:1276].reshape(1276, 1)

x_val = x_train_num[1276:].reshape(274, -1)
y_val = y_train_num[1276:].reshape(274, 1)

x_test = x_test_num.copy().reshape(274, -1)
y_test = y_test_num.copy().reshape(274, 1)

print('SHAPES')
print(f'x train: {x_train.shape} \ttest: {x_test.shape} \tval: {x_val.shape}')
print(f'y train: {y_train.shape} \ttest: {y_test.shape} \t\tval: {y_val.shape}')

"""# Normalise eegs"""

def normalise(x_mean, x_std, x_data):
  return (x_data - x_mean) / x_std

x_mean = x_train.mean()
x_std = x_train.std()

x_train = normalise(x_mean, x_std, x_train)
x_val = normalise(x_mean, x_std, x_val)
x_test = normalise(x_mean, x_std, x_test)

x_test[0]

"""# Show data"""

import matplotlib.pyplot as plt

def plot_eeg_piece(image):
  plt.figure(figsize=(20,20))
  plt.imshow(image.squeeze(), cmap=plt.get_cmap('gray'))
  plt.axis('off')
  plt.show()

rnd_idx = np.random.randint(len(y_test))
if y_test[rnd_idx, 0] == 1 : 
  print('ADHD') 
else:
  print('Control')

data = x_test_num[rnd_idx]
for i, channel in enumerate(data):
  plt.title(f"Channel {i + 1}") 
  plt.plot(channel)
  plt.show()

"""# Crear minibatches"""

def create_minibatches(x, y, mb_size, shuffle = True):
  assert x.shape[0] == y.shape[0], 'Sample size error'
  total_data = x.shape[0]
  if shuffle:
    idxs = np.arange(total_data)
    np.random.shuffle(idxs)
    x = x[idxs]
    y = y[idxs]
  return ((x[i:i+mb_size], y[i:i+mb_size]) for i in range(0, total_data, mb_size))

"""# PyTorch

## Convert numpy array to PyTorch
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

x_train_tensor = torch.tensor(x_train.copy())
y_train_tensor = torch.tensor(y_train.copy())

x_val_tensor = torch.tensor(x_val.copy())
y_val_tensor = torch.tensor(y_val.copy())

x_test_tensor = torch.tensor(x_test.copy())
y_test_tensor = torch.tensor(y_test.copy())

"""## Use GPU if available"""

if torch.cuda.is_available():
  device = torch.device('cuda')
else:
  device = torch.device('cpu')
print(f'Were using {device}')

"""## Compute accuracy

Checar si en xi = xi.to(device=device) y yi = yi.to(device=device) se usa dtype y si sí qué dtype se usa
"""

def accuracy(model, x, y, mb_size):
  #calcular costo de validación en esta funcion
    num_correct = 0
    num_total = 0
    model.eval()
    model = model.to(device=device)
    with torch.no_grad():
        for (xi, yi) in create_minibatches(x, y, mb_size):
            xi = xi.to(device=device, dtype = torch.float)
            yi = yi.to(device=device, dtype = torch.long)
            scores = model(xi) # mb_size, 2
            _, pred = scores.max(dim=1) #pred shape (mb_size )
            num_correct += (pred == yi.squeeze()).sum() # pred shape (mb_size), yi shape (mb_size, 1)
            num_total += pred.size(0)

            return float(num_correct)/num_total

"""## Train loop

Checar si en xi = xi.to(device=device) y yi = yi.to(device=device) se usa dtype y si sí qué dtype se usa
"""

def train(model, optimiser, mb_size, epochs=100):
    model = model.to(device=device)
    for epoch in range(epochs):
        for (xi, yi) in create_minibatches(x_train_tensor, y_train_tensor, mb_size):
            model.train()
            xi = xi.to(device=device, dtype=torch.float)
            yi = yi.to(device=device, dtype=torch.long)
            scores = model(xi)
            # funcion cost
            cost = F.cross_entropy(input= scores, target=yi.squeeze())
            optimiser.zero_grad()
            cost.backward()
            optimiser.step()
            
        print(f'Epoch: {epoch}, costo: {cost.item()}, accuracy: {accuracy(model, x_val_tensor, y_val_tensor, mb_size)}')

def train(model, optimiser, mb_size, epochs=100, scheduler=None):
    model = model.to(device=device)
    for epoch in range(epochs):
        for (xi, yi) in create_minibatches(x_train_tensor, y_train_tensor, mb_size):
            model.train()
            xi = xi.to(device=device, dtype=torch.float)
            yi = yi.to(device=device, dtype=torch.long)
            scores = model(xi)
            # funcion cost
            cost = F.cross_entropy(input= scores, target=yi.squeeze())
            optimiser.zero_grad()
            cost.backward()
            optimiser.step()
            if scheduler:
              scheduler.step()
            
        print(f'Epoch: {epoch}, costo: {cost.item()}, accuracy: {accuracy(model, x_val_tensor, y_val_tensor, mb_size)}')

        #regresar y graficar costos de entrenamiento y validacion

"""## Model using Sequential"""

from torch.nn.modules.linear import Linear
from torch.nn.modules.activation import ReLU

hidden1 = 1000
hidden = 1000
lr = 0.001
epochs = 150
mb_size = 16
model1 = nn.Sequential(nn.Linear(in_features=21888, out_features=hidden1), nn.ReLU(),
                       nn.Linear(in_features=hidden1, out_features=hidden), nn.ReLU(),
                       nn.Linear(in_features=hidden, out_features=2))
optimiser = torch.optim.Adam(model1.parameters(), lr=lr, weight_decay=1e-4)
scheduler = torch.optim.lr_scheduler.OneCycleLR(optimiser, lr, epochs=epochs, steps_per_epoch=80, anneal_strategy='linear', pct_start=0.4)

# train(model1, optimiser, mb_size, epochs, scheduler)
train(model1, optimiser, mb_size, epochs)

"""## Accuracy"""

accuracy(model1, x_test_tensor,  y_test_tensor, mb_size)



"""batch size mas chico, potencia dee dos (8, 16)

"""