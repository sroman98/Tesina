# -*- coding: utf-8 -*-
"""2 Find LR ADHD.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kCAs2ARCz7hWGQWS0tyIsfuYdZKZb60v
"""

# Commented out IPython magic to ensure Python compatibility.
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import numpy as np

from google.colab import drive
from os import listdir
from os.path import isfile, join
from scipy.io import loadmat
from statistics import mean
from torch.nn import functional as F
from torch.utils.data import DataLoader, Dataset, random_split

drive.mount('/gdrive')
# %cd /gdrive/MyDrive/Sandra - DL y EEGs para TDAH en niÃ±os/dataset
adhd_paths = ['ADHD_1', 'ADHD_2']
control_paths = ['Control_1', 'Control_2']

def getEEGsFromPaths(paths):
  eegs = []
  
  for path in paths:
    files = [f for f in listdir(path) if isfile(join(path, f))]

    for file in files:
      eeg_mat = loadmat(path + '/' + file)
      size = len(file)
      file_no_ext = file[:size - 4]
      eeg = eeg_mat[file_no_ext]
      eegs.append(eeg)
  
  return eegs

adhd_eegs = getEEGsFromPaths(adhd_paths)
control_eegs = getEEGsFromPaths(control_paths)

CHAN = 19
FREQ = 128 # samples / second
seconds_in_split = 1 # can change this to test
SAMPLES = FREQ * seconds_in_split

def split_list(l: list) -> np.array:
  splitedSize = SAMPLES
  wasted = len(l) % splitedSize
  return np.array([l[x:x+splitedSize] for x in range(0, len(l) - wasted, splitedSize)])

adhd_splited_arrays = np.concatenate([split_list(eeg) for eeg in adhd_eegs]) 
control_splited_arrays = np.concatenate([split_list(eeg) for eeg in control_eegs]) 

print(f'ADHD splited: {adhd_splited_arrays.shape}')
print(f'Control splited: {control_splited_arrays.shape}')

class CustomEEGDataset(Dataset):
    def __init__(self, adhd_data, control_data, transform=None, target_transform=None):
      adhd_y = [1] * len(adhd_data)
      control_y = [0] * len(control_data)

      y = np.concatenate((control_y, adhd_y), axis = 0)
      # y = y.reshape(len(y), 1)
      self.eeg_labels = y

      x = np.concatenate((control_splited_arrays, adhd_splited_arrays), axis = 0)
      x = np.transpose(x, (0, 2, 1)) # don't transpose if you want to test one sample per channel concatenated
      x = x.reshape(len(x), -1)
      self.eegs = x

      self.transform = transform
      self.target_transform = target_transform

    def __len__(self):
        return len(self.eeg_labels)

    def __getitem__(self, idx):
        eeg = self.eegs[idx]
        label = self.eeg_labels[idx]
        if self.transform:
            eeg = self.transform(eeg)
        if self.target_transform:
            label = self.target_transform(label)
        return eeg, label

dataset =  CustomEEGDataset(adhd_splited_arrays, control_splited_arrays)

SIZE = len(dataset)
TRAIN_SIZE = int(SIZE * 0.7)
VAL_SIZE = int(SIZE * 0.15)
TEST_SIZE = SIZE - TRAIN_SIZE - VAL_SIZE
MINIBATCH_SIZE = 8

train_dataset, val_dataset, test_dataset = random_split(dataset, [TRAIN_SIZE, VAL_SIZE, TEST_SIZE])

train_loader = DataLoader(train_dataset, batch_size=MINIBATCH_SIZE, shuffle = True)
val_loader = DataLoader(val_dataset, batch_size=MINIBATCH_SIZE, shuffle = True)
test_loader = DataLoader(test_dataset, batch_size=MINIBATCH_SIZE, shuffle = True)

for i, (x, y) in enumerate(test_loader):
    print(i, x.shape, y.shape)

# Plot

def plot_eeg_piece(image):
  plt.figure(figsize=(10,3))
  plt.plot(image)
  plt.show()

rnd_sample_idx = np.random.randint(len(test_loader.dataset))
print(f'La imagen muestreada representa un: {test_loader.dataset[rnd_sample_idx][1]}')
image = test_loader.dataset[rnd_sample_idx][0]
image = (image - image.min()) / (image.max() -image.min() )
plot_eeg_piece(image)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)

# Accuracy

def accuracy(model, loader):
    correct = 0
    total = 0
    cost = 0
    model.eval()
    model = model.to(device=device)
    with torch.no_grad():
        for x, y in loader:
            x = x.to(device=device, dtype=torch.float)
            y = y.to(device=device, dtype=torch.long)
            scores = model(x)
            cost += (F.cross_entropy(scores, y)).item()
            # cost += (F.cross_entropy(scores, y.squeeze())).item()
            _, pred = scores.max(dim=1)
            correct += (pred == y).sum()
            # correct += (pred == y.squeeze()).sum()
            total += pred.size(0)
        return cost/len(loader), float(correct)/total

# Buscar LR

def find_lr(model, optimiser, start_val = 1e-6, end_val = 1, beta = 0.99, loader = train_loader):
    n = len(loader) - 1
    factor = (end_val / start_val)**(1/n)
    lr = start_val
    optimiser.param_groups[0]['lr'] = lr #this allows you to update the learning rate
    avg_loss, loss, acc = 0., 0., 0.
    lowest_loss = 0.
    batch_num = 0
    losses = []
    log_lrs = []
    accuracies = []
    model = model.to(device=device)
    for i, (x, y) in enumerate(loader, start=1):
        x = x.to(device = device, dtype = torch.float)
        y = y.to(device = device, dtype = torch.long)
        optimiser.zero_grad()
        scores = model(x)
        cost = F.cross_entropy(input=scores, target=y)
        # cost = F.cross_entropy(input=scores, target=y.squeeze())
        loss = beta*loss + (1-beta)*cost.item()
        #bias correction
        avg_loss = loss/(1 - beta**i)
        
        acc_ = ((torch.argmax(scores, dim=1) == y).sum()/scores.size(0)) 
        # acc_ = ((torch.argmax(scores, dim=1) == y.squeeze()).sum()/scores.size(0)) 
#         acc = beta*acc + (1-beta)*acc_.item()
#         avg_acc = acc/(1 - beta**i)
        # if loss is massive stop
        if i > 1 and avg_loss > 4 * lowest_loss:
            print(f'from here {i, cost.item()}')
            return log_lrs, losses, accuracies
        if avg_loss < lowest_loss or i == 1:
            lowest_loss = avg_loss
        
#         accuracies.append(acc.item())
        accuracies.append(acc_.item())
#         accuracies.append(avg_acc)
        losses.append(avg_loss)
        log_lrs.append(lr)
        # step
        cost.backward()
        optimiser.step()
        # update lr
        print(f'cost:{cost.item():.4f}, lr: {lr:.4f}, acc: {acc_.item():.4f}')
        lr *= factor
        optimiser.param_groups[0]['lr'] = lr
        
    return log_lrs, losses, accuracies

# Loop de entrenamiento

def train(model, optimiser, scheduler = None, epochs = 100):
    model = model.to(device = device)
    val_loss_history = []
    train_loss_history = []
    val_acc_history = []
    train_acc_history = []
    lrs = []
    train_cost = 0.
    val_cost = 0.
    train_cost_acum = 0
    for epoch in range(epochs):
        train_correct_num  = 0
        train_total = 0
        train_cost_acum = 0
        for mb, (x, y) in enumerate(train_loader, start=1):
            model.train()
            x = x.to(device=device, dtype=torch.float)
            y = y.to(device=device, dtype=torch.long)
            scores = model(x)
            cost = F.cross_entropy(input=scores, target=y)
            # cost = F.cross_entropy(input=scores, target=y.squeeze())
            optimiser.zero_grad()
            cost.backward()
            optimiser.step()
            #if using scheduler
            if scheduler: scheduler.step()

            train_correct_num += (torch.argmax(scores, dim=1) == y).sum()    
            # train_correct_num += (torch.argmax(scores, dim=1) == y.squeeze()).sum()
            train_total += scores.size(0)        
            train_cost_acum += cost.item()
            train_acc = float(train_correct_num)/train_total  
#             train_cost = train_cost_acum/mb
            val_cost, val_acc = accuracy(model, val_loader)

            val_loss_history.append(val_cost)
            train_loss_history.append(cost.item())
            val_acc_history.append(val_acc)
            train_acc_history.append(train_acc)
#             lrs.append(scheduler.get_last_lr()[0])
            lrs.append(optimiser.param_groups[0]["lr"])
        
        #f'last lr: {scheduler.get_last_lr()[0]:6f},
        
        train_acc = float(train_correct_num)/train_total
        train_cost = train_cost_acum/len(train_loader)
        print(f'Epoch:{epoch}, train cost: {train_cost:.6f}, val cost: {val_cost:.6f},'
                      f' train acc: {train_acc:.4f}, val acc: {val_acc:4f}, total: {train_total},'
                      f' lr: {optimiser.param_groups[0]["lr"]:.6f}')
        
    return train_loss_history, val_loss_history, train_acc_history, val_acc_history, lrs

hidden1 = 1000
hidden = 1000
model1 = nn.Sequential(nn.Linear(in_features=SAMPLES * CHAN, out_features=hidden1), nn.ReLU(),
                       nn.Linear(in_features=hidden1, out_features=hidden), nn.ReLU(),
                       nn.Linear(in_features=hidden, out_features=2))
optimiser = torch.optim.SGD(model1.parameters(),
                            lr=0.1, momentum=0.95,
                            weight_decay=1e-4)
lg_lr, losses, accuracies = find_lr(model1, optimiser, start_val=1e-10, end_val=10)

# Plot losses

f1, ax1 = plt.subplots(figsize=(10,5))
ax1.plot(lg_lr, losses)
ax1.set_xscale('log')
ax1.get_xaxis().get_major_formatter().labelOnlyBase = False
f1.suptitle("Losses")
plt.show()

# Plot accuracies

f1, ax1 = plt.subplots(figsize=(10,5))
ax1.plot(lg_lr, accuracies)
ax1.set_xscale('log')
ax1.get_xaxis().get_major_formatter().labelOnlyBase = False
f1.suptitle("Accuracies")
plt.show()

# Train Model
model1 = nn.Sequential(nn.Linear(in_features=SAMPLES * CHAN, out_features=hidden1), nn.ReLU(),
                       nn.Linear(in_features=hidden1, out_features=hidden), nn.ReLU(),
                       nn.Linear(in_features=hidden, out_features=2))
optimiser = torch.optim.SGD(model1.parameters(),
                            lr=0.1, momentum=0.95,
                            weight_decay=1e-4)
epochs = 6
scheduler = torch.optim.lr_scheduler.OneCycleLR(optimiser, 
                                                max_lr=6e-5,
                                                steps_per_epoch=len(train_loader), 
                                                epochs = epochs, pct_start=0.43, 
                                                div_factor=10, 
                                                final_div_factor=1000, 
                                                three_phase=True, verbose=False
                                            )
train_loss_history, val_loss_history, train_acc_history, val_acc_history, lrs = train(
                                model1, 
                                optimiser=optimiser,
                                scheduler=scheduler,
                                epochs = epochs
                                )

# Test model

accuracy(model1, test_loader)